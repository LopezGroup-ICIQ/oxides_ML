{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generalized ML Model Analysis\n",
    "\n",
    "This notebook provides a flexible framework for analyzing trained ML model outputs and performance metrics across different training runs and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION SECTION - Modify these parameters to analyze different models\n",
    "# ============================================================================\n",
    "\n",
    "# Base path for training results\n",
    "# Set this to your training results directory or use environment variable\n",
    "BASE_PATH = os.environ.get(\n",
    "    \"TRAINING_RESULTS_DIR\",\n",
    "    \"./models/test_training\"  # Replace with your training results directory\n",
    ")\n",
    "\n",
    "# Model directory (relative to BASE_PATH)\n",
    "# Examples:\n",
    "# - \"database_1/ads_height_test_1\"\n",
    "# - \"database_2/surface_order_2_cn\"\n",
    "# - \"database_3/test_cn\"\n",
    "# - \"database_3/segmented_test\"\n",
    "MODEL_DIRECTORY = os.environ.get(\n",
    "    \"MODEL_DIRECTORY\",\n",
    "    \"database_3/test_cn\"  # Replace with your model directory\n",
    ")\n",
    "\n",
    "# Define which datasets to load (modify as needed)\n",
    "DATASETS_TO_LOAD = {\n",
    "    'training': 'training.csv',\n",
    "    'train_set': 'train_set.csv',\n",
    "    'test_set': 'test_set.csv',\n",
    "    'validation_set': 'validation_set.csv',\n",
    "    'uncertainty': 'uq.csv'\n",
    "}\n",
    "\n",
    "# Model output path\n",
    "MODEL_PATH = os.path.join(BASE_PATH, MODEL_DIRECTORY)\n",
    "\n",
    "# ============================================================================\n",
    "# NOTE: Set environment variables for different systems:\n",
    "# export TRAINING_RESULTS_DIR=/path/to/training/results\n",
    "# export MODEL_DIRECTORY=database_3/test_cn\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "print(f\"Model directory exists: {os.path.exists(MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data = {}\n",
    "for name, filename in DATASETS_TO_LOAD.items():\n",
    "    filepath = os.path.join(MODEL_PATH, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        data[name] = pd.read_csv(filepath)\n",
    "        print(f\"✓ Loaded {name}: {data[name].shape[0]} rows, {data[name].shape[1]} columns\")\n",
    "    else:\n",
    "        print(f\"✗ {filename} not found\")\n",
    "\n",
    "# Extract data frames for convenience\n",
    "df_train = data.get('training')\n",
    "df_train_set = data.get('train_set')\n",
    "df_test_set = data.get('test_set')\n",
    "df_val_set = data.get('validation_set')\n",
    "df_uq = data.get('uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display info about test set\n",
    "if df_test_set is not None:\n",
    "    print(\"Test Set Information:\")\n",
    "    print(df_test_set.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Performance Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "if df_test_set is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SET PERFORMANCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'Abs_error_eV' in df_test_set.columns:\n",
    "        mae = df_test_set['Abs_error_eV'].mean()\n",
    "        rmse = np.sqrt((df_test_set['Abs_error_eV']**2).mean())\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f} eV\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f} eV\")\n",
    "    \n",
    "    if 'Predicted_energy_eV' in df_test_set.columns and 'True_energy_eV' in df_test_set.columns:\n",
    "        from sklearn.metrics import r2_score\n",
    "        r2 = r2_score(df_test_set['True_energy_eV'], df_test_set['Predicted_energy_eV'])\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nNumber of test samples: {len(df_test_set)}\")\n",
    "    print(f\"Columns: {list(df_test_set.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "if df_train is not None:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # MAE curves\n",
    "    if 'Epoch' in df_train.columns:\n",
    "        if 'Train_MAE_eV' in df_train.columns:\n",
    "            axs[0].plot(df_train['Epoch'], df_train['Train_MAE_eV'], label='Train MAE', marker='o', markersize=3)\n",
    "        if 'Val_MAE_eV' in df_train.columns:\n",
    "            axs[0].plot(df_train['Epoch'], df_train['Val_MAE_eV'], label='Validation MAE', marker='s', markersize=3)\n",
    "        if 'Test_MAE_eV' in df_train.columns:\n",
    "            axs[0].plot(df_train['Epoch'], df_train['Test_MAE_eV'], label='Test MAE', marker='^', markersize=3)\n",
    "        \n",
    "        axs[0].set_xlabel('Epoch')\n",
    "        axs[0].set_ylabel('MAE (eV)')\n",
    "        axs[0].set_title('Learning Curves: MAE')\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss curves\n",
    "    if 'Epoch' in df_train.columns:\n",
    "        if 'Train_Loss' in df_train.columns:\n",
    "            axs[1].plot(df_train['Epoch'], df_train['Train_Loss'], label='Train Loss', marker='o', markersize=3)\n",
    "        if 'Val_Loss' in df_train.columns:\n",
    "            axs[1].plot(df_train['Epoch'], df_train['Val_Loss'], label='Validation Loss', marker='s', markersize=3)\n",
    "        \n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].set_title('Learning Curves: Loss')\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # R² or other metric\n",
    "    if 'Epoch' in df_train.columns and 'R2_Score' in df_train.columns:\n",
    "        axs[2].plot(df_train['Epoch'], df_train['R2_Score'], label='R² Score', marker='o', markersize=3, color='green')\n",
    "        axs[2].set_xlabel('Epoch')\n",
    "        axs[2].set_ylabel('R² Score')\n",
    "        axs[2].set_title('R² Score Over Training')\n",
    "        axs[2].legend()\n",
    "        axs[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Prediction vs True Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of predictions vs true values\n",
    "if df_test_set is not None and 'Predicted_energy_eV' in df_test_set.columns and 'True_energy_eV' in df_test_set.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    ax.scatter(df_test_set['True_energy_eV'], df_test_set['Predicted_energy_eV'], alpha=0.6, s=50)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(df_test_set['True_energy_eV'].min(), df_test_set['Predicted_energy_eV'].min())\n",
    "    max_val = max(df_test_set['True_energy_eV'].max(), df_test_set['Predicted_energy_eV'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('True Energy (eV)')\n",
    "    ax.set_ylabel('Predicted Energy (eV)')\n",
    "    ax.set_title('Model Predictions vs True Values')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "if df_test_set is not None and 'Abs_error_eV' in df_test_set.columns:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Histogram of absolute errors\n",
    "    axs[0, 0].hist(df_test_set['Abs_error_eV'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axs[0, 0].set_xlabel('Absolute Error (eV)')\n",
    "    axs[0, 0].set_ylabel('Count')\n",
    "    axs[0, 0].set_title('Distribution of Absolute Errors')\n",
    "    axs[0, 0].axvline(df_test_set['Abs_error_eV'].mean(), color='r', linestyle='--', label=f'Mean: {df_test_set[\"Abs_error_eV\"].mean():.3f}')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Sorted errors\n",
    "    sorted_errors = np.sort(df_test_set['Abs_error_eV'].values)\n",
    "    axs[0, 1].plot(sorted_errors, linewidth=2)\n",
    "    axs[0, 1].set_xlabel('Sample Index (sorted)')\n",
    "    axs[0, 1].set_ylabel('Absolute Error (eV)')\n",
    "    axs[0, 1].set_title('Sorted Absolute Errors')\n",
    "    axs[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error statistics\n",
    "    stats_text = f\"\"\"\n",
    "    Error Statistics:\n",
    "    Mean: {df_test_set['Abs_error_eV'].mean():.4f} eV\n",
    "    Median: {df_test_set['Abs_error_eV'].median():.4f} eV\n",
    "    Std Dev: {df_test_set['Abs_error_eV'].std():.4f} eV\n",
    "    Min: {df_test_set['Abs_error_eV'].min():.4f} eV\n",
    "    Max: {df_test_set['Abs_error_eV'].max():.4f} eV\n",
    "    95th Percentile: {df_test_set['Abs_error_eV'].quantile(0.95):.4f} eV\n",
    "    \"\"\"\n",
    "    axs[1, 0].text(0.1, 0.5, stats_text, fontsize=11, family='monospace')\n",
    "    axs[1, 0].axis('off')\n",
    "    \n",
    "    # Box plot of errors by percentile\n",
    "    error_ranges = pd.cut(df_test_set['Abs_error_eV'], bins=5, labels=['0-20%', '20-40%', '40-60%', '60-80%', '80-100%'])\n",
    "    axs[1, 1].boxplot([df_test_set[error_ranges == label]['Abs_error_eV'].values for label in ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']],\n",
    "                      labels=['0-20%', '20-40%', '40-60%', '60-80%', '80-100%'])\n",
    "    axs[1, 1].set_ylabel('Absolute Error (eV)')\n",
    "    axs[1, 1].set_title('Error Distribution by Percentile')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Material-wise Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_DIRECTORY}\")\n",
    "print(f\"Path: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
